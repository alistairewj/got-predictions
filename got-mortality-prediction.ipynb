{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import sys\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# basic natural language feature extraction\n",
    "import sklearn.feature_extraction.text as fet\n",
    "\n",
    "# used for train/test splits and cross validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# used to impute mean for data and standardize for computational stability\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# logistic regression is our favourite model ever\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# used to calculate AUROC/accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "# used to create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# gradient boosting - must download package https://github.com/dmlc/xgboost\n",
    "#import xgboost as xgb\n",
    "\n",
    "# default colours for prettier plots\n",
    "\n",
    "col = [[0.9047, 0.1918, 0.1988],\n",
    "    [0.2941, 0.5447, 0.7494],\n",
    "    [0.3718, 0.7176, 0.3612],\n",
    "    [1.0000, 0.5482, 0.1000],\n",
    "    [0.4550, 0.4946, 0.4722],\n",
    "    [0.6859, 0.4035, 0.2412],\n",
    "    [0.9718, 0.5553, 0.7741],\n",
    "    [0.5313, 0.3359, 0.6523]];\n",
    "marker = ['v','o','d','^','s','o','+']\n",
    "ls = ['-','-','-','-','-','s','--','--']\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data from the got-data.ipynb file\n",
    "# most of it is scrapped from the game of thrones tv show wiki\n",
    "df = pd.read_csv('data/got_data_valid_characters.csv', sep=',', index_col=0)\n",
    "\n",
    "# print out the data we have\n",
    "print('Total of {} characters.'.format(df.shape[0]))\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# define a matrix of relationships ... realized this is harder than it looks!\n",
    "# 0 - not related\n",
    "# 1 - father\n",
    "# 2 - mother\n",
    "# 3 - sibling\n",
    "# 4 - child\n",
    "# 5 - other\n",
    "fam = pd.DataFrame(np.zeros([df.shape[0],df.shape[0]]), index=df.index, columns=df.index)\n",
    "for i, char in enumerate(fam.index):\n",
    "    curr_fam = df.loc[char, 'Family']\n",
    "    if curr_fam is np.nan or curr_fam is None:\n",
    "        continue\n",
    "    \n",
    "    # otherwise, let's parse their family tree\n",
    "    common_fam = [filter(lambda x: x in curr_fam, sublist) for sublist in fam.index]\n",
    "    print(curr_fam)\n",
    "    #print(common_fam)\n",
    "    break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define some useful subfunctions\n",
    "\n",
    "# given a dataframe/regex phrase, this counts the number of times the regex appears\n",
    "def count_words(s, phrase):\n",
    "    if s is np.nan:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(re.findall(phrase,s, re.IGNORECASE))\n",
    "\n",
    "# this calls the above function and adds the data to the given dataframe\n",
    "def add_data(df_data, column_name, phrase, txt_col=None):\n",
    "    if txt_col is None:\n",
    "        txt_col = ['Background','Season 1','Season 2', 'Season 3', 'Season 4']\n",
    "        \n",
    "    df_new = df[txt_col].applymap(lambda x: count_words(x,phrase))\n",
    "    df_data[column_name] = df_new[txt_col].sum(axis=1)\n",
    "    return df_data\n",
    "\n",
    "def add_allegiance(df, df_has_allegiance):\n",
    "    \n",
    "    houses = ['House Lannister', 'House Stark', 'House Targaryen', 'House Frey', \n",
    "              'House Greyjoy', \"Night's Watch\", 'House Martell', 'Free Folk', 'House Forrester',\n",
    "              'House Bolton', 'House Baratheon', 'Kingsguard']\n",
    "    \n",
    "    for h in houses:\n",
    "        tmp = np.zeros(df.shape[0])\n",
    "        val = df_has_allegiance['Allegiance'].values\n",
    "        for i in range(df.shape[0]):\n",
    "            if val[i] is np.nan:\n",
    "                continue\n",
    "            elif h in val[i]:\n",
    "                tmp[i] = 1.0\n",
    "        df[h] = tmp\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data(df, s=3):\n",
    "    # given the dataframe and the season, extract some features\n",
    "    # only look at background // season 1\n",
    "    \n",
    "    # by default, extract data up to and including season 3, i.e. s=3\n",
    "    txt_col = ['Background','Season 1','Season 2','Season 3','Season 4',' Season 5']\n",
    "    \n",
    "    txt_col = txt_col[0:(s+1)]\n",
    "    \n",
    "    # initialize the dataframe\n",
    "    idxData = np.ones(df.shape[0], dtype=bool)\n",
    "    df_data = df.loc[idxData, ['Status','Season(s)']]\n",
    "\n",
    "    # start adding data based on counting the frequency of words\n",
    "\n",
    "    # count the number of hyphens as a surrogate as the number of in-show family member\n",
    "    # in the data, the family members appear as \"sister - Sansa Stark, father - Eddard Stark,\" .. etc\n",
    "    df_data = add_data(df_data, 'family_members',' - ', txt_col = ['Family'])\n",
    "\n",
    "    #df_data = add_allegiance(df_data, df)\n",
    "    \n",
    "    # number of seasons the character appeared in\n",
    "    df_data = add_data(df_data, 'number_of_seasons','[1-' + str(s) + ']', txt_col = ['Season(s)'])\n",
    "\n",
    "    # various word \"types\" across all the data\n",
    "    df_data = add_data(df_data, 'violent_words','(knight|warrior|sword|axe|spear|kill|murder|fight|assassinate)')\n",
    "    df_data = add_data(df_data, 'sexy_words','(sex|naked|love|slept|nude|kiss)')\n",
    "    df_data = add_data(df_data, 'fun_words','(beer|wine|glass|drunk|inebriate)')\n",
    "    df_data = add_data(df_data, 'number_of_words','\\w+')\n",
    "\n",
    "    # remove people who died before current season\n",
    "    idxRemove = np.zeros(df_data.shape[0],dtype=bool)\n",
    "    for i, val in enumerate(df_data['Status'].values):\n",
    "        char = df_data.index[i]\n",
    "        if val is None or val is np.nan:\n",
    "            continue\n",
    "        if ('Dead' in val) | ('Deceased' in val):\n",
    "            if len(re.findall('[' + str(s+1) + '-5]', df_data.loc[char, 'Season(s)'])) > 0:\n",
    "                continue\n",
    "            else:\n",
    "                idxRemove[i] = True\n",
    "\n",
    "    df_data = df_data.loc[~idxRemove]\n",
    "\n",
    "    df_data.drop('Season(s)',axis=1,inplace=True)\n",
    "    return df_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data = extract_data(df, s=4)\n",
    "df_data.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prep data\n",
    "X = df_data.values[:,1:df_data.shape[1]]\n",
    "\n",
    "# alive until proven dead !\n",
    "y = np.zeros(df_data.shape[0])\n",
    "for i, val in enumerate(df_data['Status'].values):\n",
    "    if val is None or val is np.nan:\n",
    "        continue\n",
    "    if 'Dead' in val:\n",
    "        y[i] = 1.0\n",
    "    elif 'Deceased' in val:\n",
    "        y[i] = 1.0\n",
    "        \n",
    "# workaround cross_val_predict returning 0s/1s - calling this makes it return probabilities\n",
    "class proba_logreg(LogisticRegression):\n",
    "    def predict(self, X):\n",
    "        return LogisticRegression.predict_proba(self, X)\n",
    "\n",
    "# cross-validation performance\n",
    "mdl = \"logreg\"\n",
    "model = LogisticRegression(fit_intercept=True)\n",
    "estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                      strategy=\"mean\",\n",
    "                                      axis=0)),\n",
    "                  (\"scaler\", StandardScaler()),\n",
    "                  (\"lr\", model)])\n",
    "scores = cross_val_score(estimator, X, y, scoring='roc_auc',cv=5)\n",
    "print('{:10s} {:5g} [{:5g}, {:5g}]'.format(\"lr\", np.mean(scores), np.min(scores), np.max(scores) ))\n",
    "\n",
    "# plot a roc curve by getting cross-validation predictions\n",
    "predicted = cross_val_predict(proba_logreg(), X, y, cv=10)\n",
    "predicted = predicted[:,1]\n",
    "\n",
    "plt.figure(figsize=[9,9])\n",
    "ax = plt.gca()\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, predicted, pos_label=1)\n",
    "plt.plot(fpr, tpr, 'bo-',lw=2,markersize=12,\n",
    "        label=mdl + ' ' + '%0.3f' % metrics.auc(fpr, tpr))\n",
    "plt.xlabel('False positive rate',fontsize=14)\n",
    "plt.ylabel('True positive rate',fontsize=14)\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Mortality prediction',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sort_indices = np.argsort(predicted)\n",
    "\n",
    "for i in range(20):\n",
    "    idx = sort_indices[-i-1]\n",
    "    print('{:20s}: {:0.5f} (outcome: {})'.format(df_data.index[idx],\n",
    "                                        predicted[idx],\n",
    "                                       y[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "Let's try TF-IDF of the same data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_data_text(df, s=3):\n",
    "    # given the dataframe and the season, extract some features\n",
    "    # only look at background // season 1\n",
    "    \n",
    "    # by default, extract data up to and including season 3, i.e. s=3\n",
    "    txt_col = ['Background','Season 1','Season 2','Season 3','Season 4',' Season 5']\n",
    "    \n",
    "    txt_col = txt_col[0:(s+1)]\n",
    "    \n",
    "    # initialize the dataframe\n",
    "    idxData = np.ones(df.shape[0], dtype=bool)\n",
    "    df_data = df.loc[idxData, ['Status','Season(s)']]\n",
    "\n",
    "    # === ADD DATA === #\n",
    "\n",
    "    tmp = None\n",
    "    for txt in txt_col:\n",
    "        if tmp is None:\n",
    "            tmp = df[txt]\n",
    "        else:\n",
    "            tmp = tmp.str.cat(df[txt].fillna('') , sep='\\n')\n",
    "\n",
    "    df_data['text'] = tmp.fillna('')\n",
    "    \n",
    "    # remove people who died before current season\n",
    "    idxRemove = np.zeros(df_data.shape[0],dtype=bool)\n",
    "    for i, val in enumerate(df_data['Status'].values):\n",
    "        char = df_data.index[i]\n",
    "        if val is None or val is np.nan:\n",
    "            continue\n",
    "        if ('Dead' in val) | ('Deceased' in val):\n",
    "            if len(re.findall('[' + str(s+1) + '-5]', df_data.loc[char, 'Season(s)'])) > 0:\n",
    "                continue\n",
    "            else:\n",
    "                idxRemove[i] = True\n",
    "\n",
    "    df_data = df_data.loc[~idxRemove]\n",
    "    \n",
    "    # now we have the data frame ... get X and y matrices\n",
    "    X = df_data.values[:,1:df_data.shape[1]]\n",
    "\n",
    "    # alive until proven dead !\n",
    "    y = np.zeros(df_data.shape[0])\n",
    "    for i, val in enumerate(df_data['Status'].values):\n",
    "        if val is None or val is np.nan:\n",
    "            continue\n",
    "        # check if they have any seasons after the next one\n",
    "        # this is to confirm that, if they died, they died in the next season\n",
    "        if len(re.findall('[' + str(s+2) + '-' + str(np.max([s+2,5])) + ']',\n",
    "                          df_data.iloc[i]['Season(s)'])) == 0:\n",
    "            if 'Dead' in val:\n",
    "                y[i] = 1.0\n",
    "            elif 'Deceased' in val:\n",
    "                y[i] = 1.0\n",
    "        \n",
    "    #print('Died in season {}'.format(s+1))\n",
    "    #for i,val in enumerate(y):\n",
    "    #    if val==1.0:\n",
    "    #        print('{}'.format(df_data.index[i]))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = extract_data_text(df, s=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf = fet.TfidfVectorizer(analyzer='word', ngram_range=(1,3), min_df = 0, stop_words = 'english')\n",
    "tfidf_matrix =  tf.fit_transform(df_data['text'])\n",
    "feature_names = tf.get_feature_names()\n",
    "print(len(feature_names))\n",
    "print('Example phrases:')\n",
    "for i in range(5):\n",
    "    print(feature_names[i+50])\n",
    "    \n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = fet.TfidfVectorizer(analyzer='word', ngram_range=(1,3), min_df = 0, stop_words = 'english')\n",
    "mdl = LassoCV(cv=5,fit_intercept=True)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', tf),\n",
    "    ('clf', mdl),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
